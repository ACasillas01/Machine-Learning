{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as g:    \n",
    "    # Grabar las operacioes en el contexto de GradientTape\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso Y es la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dy_dx = g.gradient(y,x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un módulo es un contenedor con nombre para variables, otros módulos y funciones que se aplican a los datos del usuario.\n",
    "Se ejecuta init al inicar una instancia, y call cuando esta es ejectudada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del un modelo de regresión linear simple\n",
    "class SimpleLinearRegression(tf.Module):\n",
    "    def __init__(self, input_dim = 1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        #Definir e inicializar parámetros\n",
    "        rand_w = tf.random.normal([input_dim])\n",
    "        rand_b = tf.random.normal([1])\n",
    "        self.w = tf.Variable(rand_w, name='w', dtype=tf.float32, trainable=True)\n",
    "        self.b = tf.Variable(rand_b, name='b', dtype=tf.float32, trainable=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = tf.multiply(x, self.w) + self.b\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ -8.301536 -11.801718 -20.552172 -27.552536 -34.552902], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Creaion de un modelo de regresión lineal\n",
    "lin_reg = SimpleLinearRegression(input_dim=1, name=\"Mi_Modelo\")\n",
    "# En este momento no se ha entrenado\n",
    "\n",
    "#Utilcemos los datos del ejercicio anterior de predicción de masas\n",
    "tiempo = tf.constant([5,7,12,16,20], dtype=tf.float32)\n",
    "masa = tf.constant([40,120,180,210,240], dtype= tf.float32)\n",
    "\n",
    "\n",
    "Yp = lin_reg(tiempo.numpy()) # Yp = Y predicha\n",
    "print(Yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retomamos la función de costo y calcular los gradientes respecto a un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lin_reg.trainable_variables)\n",
    "\n",
    "# Función de costo MSE\n",
    "def mse_loss(Y, Yp):\n",
    "    e = Y - Yp\n",
    "    se = tf.multiply(e,e)\n",
    "    mse = tf.reduce_mean(se, axis=0)\n",
    "    return mse\n",
    "\n",
    "#loss = mse_loss(masa, r)\n",
    "#print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0:[-1.750091], b: [0.44891912], costo: 38347.28125\n",
      "\n",
      "w0:[-1.2355852], b: [0.48462954]\n",
      "\n",
      "w0:[-1.2355852], b: [0.48462954], costo: 35734.08203125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cálculo del gradiente con respecto a un \n",
    "learining_rate = 0.0001\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Paso 1: Predicción\n",
    "    Yp = lin_reg(tiempo)\n",
    "    # Paso 2: Calcular Costo\n",
    "    loss = mse_loss(masa, Yp)\n",
    "    print(f\"w0:{lin_reg.w.numpy()}, b: {lin_reg.b.numpy()}, costo: {loss}\\n\")\n",
    "    # Paso 3: Calcular el gradiente\n",
    "    gradients = tape.gradient(loss, lin_reg.trainable_variables)\n",
    "    # Aplicar (regla de actualización de) los gradientes en la actualización de parámetros\n",
    "    for g, v in zip(gradients, lin_reg.trainable_variables):\n",
    "        v.assign_sub(learining_rate*g) # One Step\n",
    "\n",
    "    print(f\"w0:{lin_reg.w.numpy()}, b: {lin_reg.b.numpy()}\\n\")\n",
    "\n",
    "\n",
    "    Yp = lin_reg(tiempo)\n",
    "    loss = mse_loss(masa, Yp)\n",
    "    print(f\"w0:{lin_reg.w.numpy()}, b: {lin_reg.b.numpy()}, costo: {loss}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Cost =~ 485\n",
    "\n",
    "w0 & b = 12.7428 & 1.834\n",
    "\n",
    "N =~ 50?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posibles Problemas:\n",
    "\n",
    "- Mínimos Locales\n",
    "- Saddle Points (Derivada ~= 0)\n",
    "- Mismos parámetros nos llevarán por la misma dirección\n",
    "- Los gradientes que calculamos pueden tener ruido.\n",
    "\n",
    "\n",
    "Resolver Sobreentrnamiento:\n",
    "- Batch\n",
    "- Mini batch\n",
    "- Forma Estocástica"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
